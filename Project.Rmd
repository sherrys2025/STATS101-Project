---
title: "STATS 101 Project"
author: "Sherry Shen, Lisa Zhong, Selina Zhang"
output:
  pdf_document: default
---

```{r setup}
set.seed(123)     #setting seed
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```

##Data preprocessing

```{r}
data <- read.csv("ames2000_NAfix.csv", stringsAsFactors = TRUE)
data$MS.SubClass = factor(data$MS.SubClass)
```

```{r, label="Ordinal Variable Transformation"}
Convert_ordinal <- function(dict, row) {
  data[, row] = as.character(data[, row])
  for (i in 1:2000) {
     data[i, row] = dict[data[i, row]]
  }
  return (as.integer(data[, row])) # ordinal to their values
}

Lot.Shape.Order = c("IR3" = "1", "IR2" = "2", "IR1" = "3", "Reg" = "4")
data[, 7] <- Convert_ordinal(Lot.Shape.Order, 7)

Land.Slope.Order = c("Sev" = "1", "Mod" = "2", "Gtl" = "3")
data[, 11] <- Convert_ordinal(Land.Slope.Order, 11)

# data$Overall.Qual = factor(data$Overall.Qual)
# data$Overall.Cond = factor(data$Overall.Cond)

Qual.Order = c("Po" = "1", "Fa" = "2", "TA" = "3", "Gd" = "4", "Ex" = "5")
data[, 27] <- Convert_ordinal(Qual.Order, 27) # Exter.Qual to numerical order
data[, 28] <- Convert_ordinal(Qual.Order, 28) # Exter.Cond to numerical order
data[, 30] <- Convert_ordinal(Qual.Order, 30) # Bsmt.Qual to numerical order
data[, 31] <- Convert_ordinal(Qual.Order, 31) # Bsmt.Cond to numerical order
data[, 40] <- Convert_ordinal(Qual.Order, 40) # HeatingQC to numerical order
data[, 53] <- Convert_ordinal(Qual.Order, 53) # KitchenQual to numerical order
data[, 57] <- Convert_ordinal(Qual.Order, 57) # FireplaceQu to numerical order
data[, 63] <- Convert_ordinal(Qual.Order, 63) # Garage.Qual to numerical order
data[, 64] <- Convert_ordinal(Qual.Order, 64) # Garage.Cond to numerical order
data[, 72] <- Convert_ordinal(Qual.Order, 72) # Pool.QC to numerical order

Bsmt.Exposure.Order = c("No" = "1", "Mn" = "2", "Av" = "3", "Gd" = "4")
data[, 32] <- Convert_ordinal(Bsmt.Exposure.Order, 32) #Bsmt.Exposure to numerical order

BsmtFin.Order = c("Unf" = "1", "LwQ" = "2", "Rec" = "3", "BLQ" = "4", "ALQ", "5", "GLQ" = "6")
data[, 33] <- Convert_ordinal(BsmtFin.Order, 33) # BsmtFin.Type.1 to numerical order
data[, 35] <- Convert_ordinal(BsmtFin.Order, 35) # BsmtFin.Type.2 to numerical order

Electrical.Order = c("Mix" = "1", "FuseP" = "2", "FuseF" = "3", "FuseA" = "4", "SBrkr" = "5")
data[, 42] <- Convert_ordinal(Electrical.Order, 42) # Electrical to numerical order

Functional.Order = c("Sal" = "1", "Sev" = "2", "Maj2" = "3", "Maj1" = "4", "Mod" = "5", "Min2" = "6", "Min1" = "7", "Typ" = "8")
data[, 55] <- Convert_ordinal(Functional.Order, 55) # Functional to numerical order

Garage.Finish.Order = c("Unf" = "1", "RFn" = "2", "Fin" = "3")
data[, 60] <- Convert_ordinal(Garage.Finish.Order, 60) # Garage Finish to numerical order

Paved.Drive.Order = c("N" = "1", "P" = "2", "Y" = "3")
data[, 65] <- Convert_ordinal(Paved.Drive.Order, 65) # Paved Drive to numerical order

Fence.Order = c("MnWw" = "1", "GdWo" = "2", "MnPrv" = "3", "GdPrv" = "4")
data[, 73] <- Convert_ordinal(Fence.Order, 73) # Fence to numerical order
```

```{r, label = "Continuous transformation"}
# Bsmt.Full.Bath and the following variables should be factored or kept as numerical?

data$Lot.Frontage = as.integer(data$Lot.Frontage)
data$Mas.Vnr.Area = as.integer(data$Mas.Vnr.Area)
data$BsmtFin.SF.1 = as.integer(data$BsmtFin.SF.1)
data$BsmtFin.SF.2 = as.integer(data$BsmtFin.SF.2)
data$Bsmt.Unf.SF = as.integer(data$Bsmt.Unf.SF)
data$Total.Bsmt.SF = as.integer(data$Total.Bsmt.SF)
data$Garage.Yr.Blt = as.integer(data$Garage.Yr.Blt)
data$Garage.Area = as.integer(data$Garage.Area)

```

```{r}
for (y in 1:ncol(data)) {
  for (x in 1:nrow(data)) {
    if (!is.na(data[x,y])){
      if (as.character(data[x, y]) == "None") {
        data[x, y] = NA
      }
    }
  }
}
```

```{r, title = "Removing certain features"}

sale_price <- data[, 80] # extracting sale price (y)

to_remove = c(5, 6, 8, 9, 11, 14, 22, 39, 41, 42, 45, 48, 52, 63, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75)

data <- data[,-to_remove] # removing the ones we don't want


```

##Splitting the data

```{r, label = "Splitting data"}
smp_size = floor(0.5 * nrow(data))
train_Index = sample(seq_len(nrow(data)), size = smp_size)
trainSale <- sale_price[train_Index]
testSale <- sale_price[-train_Index]
trainData <- data[train_Index, ]
testData <- data[-train_Index, ] 

treeTrainData = trainData
treeTestData = testData
```

##Corrplot

```{r, label = "Corrplot on cont. var"}
require(corrplot)
cont_index <- c()
for (i in 1:ncol(trainData)) {
  if (!is.factor(data[, i])) {
    cont_index = c(cont_index, i)
  }
}

for (y in cont_index) {
  for (x in 1:length(trainData[, y])) {
    if (is.na(trainData[x, y])){
      trainData[x, y] = mean(trainData[, y], na.rm = TRUE)
    }
  }
}

correlation <- cor(trainData[cont_index])
corrplot(correlation, method = "color", tl.cex = 0.5)

```

## Removing insig. correlation cont. var

```{r, label = "Removing insignificant corr. by corr.test" }
library('psych')
corr_p = c()
remove_p = c()
for (i in cont_index[1:length(cont_index)-1]) {
  corr_p = corr.test(trainSale, trainData[, i], method = "pearson", alpha = 0.05)
  if (corr_p$p.adj > 0.05) {
    remove_p = c(remove_p, i)
  }
}

trainData = trainData[-remove_p]
```

##Evaluating spread for cat. features

```{r, label = "Visualizing the spread of cat. var and removing some with reasoning mentioned on the write up"}

factors = c()
for(j in 1:ncol(trainData)){
  if(is.factor(trainData[,j])) {
    factors = c(factors, j)
  }
}


par(mfrow=c(2,2))
for (j in factors) {
  boxplot(SalePrice ~ trainData[, j], data = trainData, 
          main = paste("Sale Price by", names(trainData)[j]),
          xlab = names(trainData)[j],
          col = "darkolivegreen2")
}

remove_cat = c(6, 7, 8, 13, 17)
trainData = trainData[-remove_cat]

```

##Evaluating spread for cont. features

```{r, label = "Visualizing the spread of cont. var and removing some with reasoning mentioned on the write up"}

cont = c()
for(j in 1:ncol(trainData)){
  if(is.numeric(trainData[,j])) {
    cont = c(cont, j)
  }
}

par(mfrow = c(2,2))
names = colnames(trainData)
for (i in cont) {
  hist(trainData[,i], main = names[i], 
       xlab = names[i], 
       col = "darkolivegreen2")
}

remove_con = c(2, 7, 8, 9, 10, 11, 16, 19, 23, 25, 26, 27, 28)
trainData = trainData[-remove_con]

```

##Initial linear model

```{r, label = "initial linear model to further reduce features"}
initial_lm <- lm(SalePrice ~ ., data = trainData)
summary(initial_lm)

```

##Initial linear model

```{r, label = "initial linear model to further reduce features"}

newTrainData = subset(trainData, select = c(MS.SubClass, Lot.Frontage, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

lm <- lm(SalePrice ~ ., data = newTrainData)
summary(lm)

```

```{r, label = "Evaluating the model"}
par(mfrow=c(2,2))
plot(lm)

#remove 1234 because it's all na
train_Index = train_Index[-which(train_Index == 1234 | train_Index == 581)]
newTrainData = subset(data[train_Index, ], select = c(MS.SubClass, Lot.Frontage, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))
```

```{r, label = "re-evaluating the model without observation 1234"}
lm2 <- lm(SalePrice ~ ., data = newTrainData)
summary(lm2)

par(mfrow=c(2,2))
plot(lm2)
```

```{r, label = "vif testing for collinearity"}
library(car)
vif = vif(lm2)
vif

```

##testing the linear model on the test dataset
```{r}
#making sure newTestData is formatted correctly, replacing nas with means of column
newTestData = subset(testData, select = c(MS.SubClass, Lot.Frontage, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

cont_index_test <- c()
for (i in 1:ncol(newTestData)) {
  if (!is.factor(newTestData[, i])) {
    cont_index_test = c(cont_index_test, i)
  }
}

for (y in cont_index_test) {
  for (x in 1:length(newTestData[, y])) {
    if (is.na(newTestData[x, y])){
      newTestData[x, y] = mean(newTestData[, y], na.rm = TRUE)
    }
  }
}


predictions <- predict(lm2, newdata = newTestData)

#calculating R^2
actuals <- testData$SalePrice
m_actuals <- mean(actuals)
ss_total <- sum((actuals - m_actuals)^2)
ss_residual <- sum((actuals - predictions)^2, na.rm = TRUE)
rsquared <- 1 - (ss_residual / ss_total)
rsquared


```
##confint and prediction interval for linear model

```{r}
confint(lm2, level = 0.95)
predInt1 = predict(lm2, newdata = newTestData, interval = "predict")

```

```{r, label = "linear model but with transformations - log"}
log_lm <- lm(log(SalePrice) ~ ., data = trainData)
summary(log_lm)

newTrainData = subset(data[train_Index, ], select = c(MS.SubClass, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

log_lm2 <- lm(log(SalePrice) ~ ., data = newTrainData)
summary(log_lm2)

par(mfrow=c(2,2))
plot(log_lm)

par(mfrow=c(2,2))
plot(log_lm2)

MSE.log <- mean(log_lm2$residuals^2)
print(MSE.log)
MSE.linear <- mean(lm2$residuals^2)
print(MSE.linear)

#testing
newTestData = subset(testData, select = c(MS.SubClass, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

cont_index_test <- c()
for (i in 1:ncol(newTestData)) {
  if (!is.factor(newTestData[, i])) {
    cont_index_test = c(cont_index_test, i)
  }
}

for (y in cont_index_test) {
  for (x in 1:length(newTestData[, y])) {
    if (is.na(newTestData[x, y])){
      newTestData[x, y] = mean(newTestData[, y], na.rm = TRUE)
    }
  }
}

predictions_log <- predict(log_lm2, newdata = newTestData)


#calculating R^2
actualsLG <- log(newTestData$SalePrice)
m_actualsLG <- mean(actualsLG)
ss_total <- sum((actualsLG - m_actualsLG)^2)
ss_residual <- sum((actualsLG - predictions_log)^2, na.rm = TRUE)
rsquared <- 1 - (ss_residual / ss_total)
rsquared

predInt2= predict(log_lm2, newdata = newTestData, interval = "predict")


```
```{r, label = "linear model but with transformations - squared and squared root"}
squared_lm <- lm((SalePrice)^2 ~ ., data = trainData)
summary(squared_lm)

sqrt_lm <- lm(sqrt(SalePrice) ~ ., data = trainData)
summary(sqrt_lm)

#train_Index = train_Index[- which(train_Index == 1208 | train_Index == 983)]

newTrainData = subset(data[train_Index, ], select = c(MS.SubClass, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

sqrt_lm2 <- lm(sqrt(SalePrice) ~ ., data = newTrainData)
summary(sqrt_lm2)

par(mfrow=c(2,2))
plot(sqrt_lm)

par(mfrow=c(2,2))
plot(sqrt_lm2)

MSE.log <- mean(log_lm2$residuals^2)
print(MSE.log)
MSE.linear <- mean(lm2$residuals^2)
print(MSE.linear)

#testing
newTestData = subset(testData, select = c(MS.SubClass, Exter.Qual, Bsmt.Qual, BsmtFin.Type.1, Fireplaces, Kitchen.Qual, TotRms.AbvGrd, Garage.Cars, Wood.Deck.SF, SalePrice))

cont_index_test <- c()
for (i in 1:ncol(newTestData)) {
  if (!is.factor(newTestData[, i])) {
    cont_index_test = c(cont_index_test, i)
  }
}

for (y in cont_index_test) {
  for (x in 1:length(newTestData[, y])) {
    if (is.na(newTestData[x, y])){
      newTestData[x, y] = mean(newTestData[, y], na.rm = TRUE)
    }
  }
}

predictions_sqrt <- predict(sqrt_lm2, newdata = newTestData)


#calculating R^2
actualsSQRT <- sqrt(newTestData$SalePrice)
m_actualsSQRT <- mean(actualsSQRT)
ss_total <- sum((actualsSQRT - m_actualsSQRT)^2)
ss_residual <- sum((actualsSQRT - predictions_sqrt)^2, na.rm = TRUE)
rsquared <- 1 - (ss_residual / ss_total)
rsquared

predInt3 = predict(sqrt_lm2, newdata = newTestData, interval = "predict")


```

##Decision tree model
```{r}
require(tree)
trainTree <- tree(SalePrice ~., treeTrainData)
summary(trainTree)

plot(trainTree)
text(trainTree, pretty = 0, cex=0.5)

predictionsTree = predict(trainTree, treeTestData)

#pruning
cv.train <- cv.tree(trainTree)
plot(cv.train$size, cv.train$dev, type = "b")
pruneTrain <- prune.tree(trainTree, best = 5)
plot(pruneTrain)
text(pruneTrain, pretty = 0)

```

##Calculating R^2 + MSE for unpruned 
```{r}
#computing R^2
actualsT <- treeTestData$SalePrice
m_actuals <- mean(actualsT)
ss_total <- sum((actualsT - m_actuals)^2)
ss_residual <- sum((actualsT - predictionsTree)^2, na.rm = TRUE)
rsquared <- 1 - (ss_residual / ss_total)
rsquared


#computing test mse
yhat <- predict(trainTree, newdata = treeTestData[1:55])
actualPrice <- treeTestData$SalePrice
plot(yhat, actualPrice)
abline(0, 1)
test.mse = mean((yhat - actualPrice)^2)
rmse = sqrt(test.mse)
print(test.mse)
print(rmse)

#this model leads to test predictions that are (on average) within approximately $58181.69$ of the true median home value for the census tract.

predInt4 = predict(trainTree, newdata = treeTestData, interval = "predict")


```

##Calculating R^2 + MSE for pruned 

```{r}

yhat <- predict(pruneTrain, newdata = treeTestData[1:55])

actualsTP <- treeTestData$SalePrice
m_actuals <- mean(actualsTP)
ss_total <- sum((actualsTP - m_actuals)^2)
ss_residual <- sum((actualsTP - yhat)^2, na.rm = TRUE)
rsquared <- 1 - (ss_residual / ss_total)
rsquared



test.prune <- treeTestData$SalePrice
plot(yhat, test.prune)
abline(0, 1)
test.mse.prune = mean((yhat - test.prune)^2)
rmse.prune = sqrt(test.mse.prune)
print(test.mse.prune)
print(rmse.prune)

#this model leads to test predictions that are (on average) within approximately $64058.58$ of the true median home value for the census tract. Pruning gets rid of noise-y things, so the performance is "worse" but more general.

predInt5 = predict(pruneTrain, newdata = treeTestData, interval = "predict")

```
#Comparing prediction intervals between models
```{r}
paste("Pred. int for lm model: ", mean(predInt1[,1], na.rm = TRUE), "to", mean(predInt1[,3], na.rm = TRUE))
paste("Pred. int for log. lm model: ", mean(predInt2[,1], na.rm = TRUE), "to", mean(predInt2[,3], na.rm = TRUE))
paste("Pred. int for sqrt. model: ", mean(predInt3[,1], na.rm = TRUE), "to", mean(predInt3[,3], na.rm = TRUE))





```
